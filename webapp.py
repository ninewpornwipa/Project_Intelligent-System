import streamlit as st
import pandas as pd
import numpy as np
import joblib
import random
import matplotlib.pyplot as plt
import os
from sklearn.preprocessing import MinMaxScaler

# Load Machine Learning Models
logistic_model = joblib.load("logistic_regression_model.pkl")
random_forest_model = joblib.load("random_forest_model.pkl")
svm_model = joblib.load("svm_model.pkl")


# à¸ªà¸£à¹‰à¸²à¸‡ Sidebar Menu
st.sidebar.title("ğŸ“ŒMenu")
page = st.sidebar.radio(
    "ğŸ” Select menu",
    [
        "ğŸ“– Machine Learning",
        "ğŸ“Š Water Potability ",
        "ğŸ¤– Neural Network",
        "ğŸŒ¡ï¸ Temperature Forecast"
    ]
)






st.title("Machine Learning & Neural Network Web ApplicationğŸ“Š")



#page 1
if page == "ğŸ“– Machine Learning":
    st.header("Machine Learning Overview")
    
    st.write("""
    
## 1ï¸âƒ£ Data Preparation

### ğŸ”¹ Loading Data
- Used `pandas` to read data from the file `water_potability.csv`.
- Load dataset from: [Kaggle](https://www.kaggle.com/).
- Inspected the dataset to understand the structure and types of data.

### ğŸ”¹ Handling Missing Values
- Used `fillna()` method to replace missing values with the mean of each column.
- Ensures data completeness and reduces bias caused by missing values.
- Checked for outliers using **Boxplot** and handled extreme values.

### ğŸ”¹ Separating Features and Labels
- **X (Features):** pH, Hardness, Solids, Chloramines, Conductivity, etc.
- **y (Label):** Potability (0 = Not Drinkable, 1 = Drinkable).
- Checked feature correlation to remove redundant variables.

### ğŸ”¹ Train-Test Split
- Used `train_test_split()` to split the dataset (80% training, 20% testing).
- Applied `stratify=y` to maintain class distribution.
- Implemented `StratifiedKFold()` for balanced data splitting.

### ğŸ”¹ Feature Scaling
- Used `StandardScaler()` to normalize feature values.
- Applied `fit_transform()` on training data and `transform()` on test data.
- Helps improve model performance by standardizing feature ranges.

---

## 2ï¸âƒ£ Theory of Algorithms Used

### ğŸ”¹ 1. Random Forest Classifier ğŸŒ²
- Uses an ensemble of **Decision Trees** for robust classification.
- Reduces overfitting and performs well on datasets with multiple important features.
- **Weakness:** Can be slow with large datasets.

### ğŸ”¹ 2. Logistic Regression ğŸ“Š
- Uses a **sigmoid function** for binary classification.
- Helps in understanding **feature importance**.
- **Weakness:** Performs poorly on non-linearly separable data.

### ğŸ”¹ 3. Support Vector Machine (SVM) ğŸ“‰
- Finds the **optimal hyperplane** for classification.
- Uses **Kernel Trick** to handle **non-linear data**.
- **Weakness:** Computationally expensive for large datasets.

---

## 3ï¸âƒ£ Model Development Steps

### âœ… Training the Model
- Used `RandomForestClassifier(n_estimators=100, random_state=42)`.
- Trained using `rf_model.fit(X_train, y_train)`.
- Applied **GridSearchCV** to fine-tune hyperparameters.

### âœ… Making Predictions
- Used `predict()` to generate predictions on test data.
- Stored predictions for evaluation.

### âœ… Evaluating the Model
- Used `accuracy_score` and `classification_report`.
- Evaluated **Precision, Recall, and F1-score**.
- Used **Confusion Matrix** for better insights into classification errors.

### âœ… Comparing Model Performance
- Compared **Random Forest, Logistic Regression, and SVM**.
- Analyzed **ROC-AUC Score** and **Precision-Recall Curve**.
- Determined the best model based on real-world generalization ability.

---
 """)
    st.write("""
    
    ### ğŸ”— References
    - Dataset Source: [Kaggle](https://www.kaggle.com/)
    - Scikit-learn Documentation: [Scikit-learn](https://scikit-learn.org/stable/documentation.html)
    """)
# ----------------------------------------------------------
    
#page 2
if page == "ğŸ“Š Water Potability ":
    st.subheader("ğŸ” Water Potability (Machine Learning)")
    df = pd.read_csv("water_potability.csv")
    feature_ranges = {col: (df[col].min(), df[col].max()) for col in df.columns if col != "Potability"}
    model_options = {
        "Logistic Regression": logistic_model,
        "Random Forest": random_forest_model,
        "SVM": svm_model
    }
    features = list(feature_ranges.keys())
    for feature in features:
        if feature not in st.session_state:
            st.session_state[feature] = round(df[feature].mean(), 2)
    if st.button("Random Feature - Unsafe Water"):
        for feature in features:
            min_val, max_val = feature_ranges[feature]
            if random.random() < 0.7:  # 70% probability to generate unsafe water
                st.session_state[feature] = round(random.uniform(min_val, min_val + (max_val - min_val) * 0.3), 2)
            else:
                st.session_state[feature] = round(random.uniform(min_val, max_val), 2)
    input_data = []
    for feature in features:
        value = st.number_input(
            f"{feature}",
            min_value=feature_ranges[feature][0],
            max_value=feature_ranges[feature][1],
            value=st.session_state[feature],
            key=f"feature_{feature}"
        )
        st.session_state[feature] = value
        input_data.append(value)
    if st.button("Prediction "):
        predictions = {}
        results = []
        for model_name, model in model_options.items():
            prediction = model.predict(np.array([input_data]))[0]
            result_text = "âœ… Drinkable Water" if prediction == 1 else "âŒ I Can't Drink Water."
            predictions[model_name] = result_text
            results.append((model_name, prediction))
        st.success("ğŸ“Š **Results From All Models**")
        for model_name, result_text in predictions.items():
            st.info(f"ğŸ¤– **{model_name}**: {result_text}")
        st.subheader("ğŸ“ˆ Comparison Of the Results Of Each Model")
        model_names = [r[0] for r in results]
        prediction_values = [r[1] for r in results]
        fig, ax = plt.subplots()
        ax.bar(model_names, prediction_values, color=['blue', 'green', 'red'])
        ax.set_ylabel("à¸œà¸¥à¸à¸²à¸£à¸à¸¢à¸²à¸à¸£à¸“à¹Œ (0 = à¸™à¹‰à¸³à¸”à¸·à¹ˆà¸¡à¹„à¸¡à¹ˆà¹„à¸”à¹‰, 1 = à¸™à¹‰à¸³à¸”à¸·à¹ˆà¸¡à¹„à¸”à¹‰)")
        ax.set_title("à¸œà¸¥à¸à¸²à¸£à¸à¸¢à¸²à¸à¸£à¸“à¹Œà¸ˆà¸²à¸à¹à¸•à¹ˆà¸¥à¸°à¹‚à¸¡à¹€à¸”à¸¥")
        ax.set_ylim(0, 1.5)
        for i, v in enumerate(prediction_values):
            ax.text(i, v + 0.05, str(v), ha='center', fontsize=12)
        st.pyplot(fig)

#page 3
if page == "ğŸ¤– Neural Network":
    st.header("Neural Network Overview")
    st.write("""
    ### ğŸ“Š Data Preparation
    - Read dataset `weather_rain_modified.csv` from [Kaggle](https://www.kaggle.com/).
    - Filled missing values using column-wise mean.
    - Applied Min-Max Scaling for normalization.
    - Split dataset into **80% Training** and **20% Testing**.

    ### ğŸ“š Algorithm Theory
    - Implemented **Fully Connected Neural Network** with TensorFlow/Keras.
    - Used **ReLU Activation** in hidden layers, **Linear Activation** in the output layer.
    - Optimized using **Adam Optimizer** and trained with **Backpropagation & Gradient Descent**.

    ### ğŸ›  Model Development Steps
    ```python
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(64, activation="relu", input_shape=(X_train.shape[1],)),
        tf.keras.layers.Dense(64, activation="relu"),
        tf.keras.layers.Dense(1, activation="linear")
    ])
    model.compile(optimizer="adam", loss="mse", metrics=["mae"])
    history = model.fit(X_train, y_train, epochs=50, batch_size=16, validation_data=(X_test, y_test))
    ```

    ### ğŸ“ˆ Model Evaluation
    ```python
    loss, mae = model.evaluate(X_test, y_test)
    print(f"Test Loss: {loss}, Test MAE: {mae}")
    ```

    ### ğŸ¯ Model Prediction
    ```python
    predictions = model.predict(X_test)
    ```

    âœ… **Neural Network is Ready for Temperature Prediction!**
    """)
    st.write("""
    ---
    ### ğŸ”— References
    - Dataset Source: [Kaggle](https://www.kaggle.com/)
    - TensorFlow/Keras Documentation: [TensorFlow](https://www.tensorflow.org/api_docs)
    """)




if page == "ğŸŒ¡ï¸ Temperature Forecast":
    model_path = r"C:\Users\Asus\Desktop\Project_IS\Temperature_Forecast.pkl"

    # ğŸ”¹ à¹‚à¸«à¸¥à¸”à¹‚à¸¡à¹€à¸”à¸¥
    if os.path.exists(model_path):
        model = joblib.load(model_path)
    else:
        model = None

    # ğŸ”¹ à¹‚à¸«à¸¥à¸” MinMaxScaler
    scaler_X = MinMaxScaler()
    scaler_y = MinMaxScaler()
    scaler_X.fit([[0, 0, 0, 900], [100, 50, 100, 1100]])
    scaler_y.fit([[10], [40]])

    st.subheader("ğŸŒ¡ï¸ Temperature Forecast (Neural Network)")

    st.write("Please enter the values to predict the temperature (Â°C)")

    # ğŸ”¹ à¸Šà¹ˆà¸­à¸‡à¸à¸£à¸­à¸à¸‚à¹‰à¸­à¸¡à¸¹à¸¥
    humidity = st.number_input("Humidity", min_value=30.0, max_value=100.0, step=1.0)
    wind_speed = st.number_input("Wind Speed", min_value=1.0, max_value=50.0, step=1.0)
    cloud_cover = st.number_input("Cloud Cover", min_value=1.0, max_value=100.0, step=1.0)
    pressure = st.number_input("Pressure", min_value=900.0, max_value=1100.0, step=1.0)


        # âœ… à¸›à¸¸à¹ˆà¸¡à¸—à¸³à¸™à¸²à¸¢
    if st.button("Predict"):
            input_data = np.array([[humidity, wind_speed, cloud_cover, pressure]])
            input_data_scaled = scaler_X.transform(input_data)

            # ğŸ”¹ à¹ƒà¸Šà¹‰à¹‚à¸¡à¹€à¸”à¸¥à¸—à¸³à¸à¸²à¸£à¸à¸¢à¸²à¸à¸£à¸“à¹Œ
            prediction_scaled = model.predict(input_data_scaled)

            # ğŸ”¹ à¸—à¸³ Inverse Transform à¹ƒà¸«à¹‰à¸„à¹ˆà¸²à¸à¸¥à¸±à¸šà¸¡à¸²à¹€à¸›à¹‡à¸™à¸­à¸‡à¸¨à¸²à¹€à¸‹à¸¥à¹€à¸‹à¸µà¸¢à¸ª
            prediction = scaler_y.inverse_transform(prediction_scaled.reshape(-1, 1))

            # ğŸ”¹ à¹à¸ªà¸”à¸‡à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œ
            st.success(f"ğŸŒ¡ï¸ Predicted Temperature: **{prediction[0][0]:.2f} Â°C**")
        # ğŸ”— References
            st.write("---")
            st.write("### ğŸ”— References")
            st.write("- Dataset Source: [Kaggle](https://www.kaggle.com/)")
            st.write("- TensorFlow/Keras Documentation: [TensorFlow](https://www.tensorflow.org/api_docs)")